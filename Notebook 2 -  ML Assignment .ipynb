{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification - 20 Newsgroups Dataset (Notebook 2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab uses the 20 Newsgroups dataset directly available in Scikit-Learn. It comprises around 18,000 newsgroups posts spread across 20 different news classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I, Samer Haidar, affirm that I completed this assignment on my own without receiving or giving any help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import processing as pp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_scores = pd.read_pickle(\"voting_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>Voting 1</td>\n",
       "      <td>Voting 2</td>\n",
       "      <td>Voting 3</td>\n",
       "      <td>Voting 4</td>\n",
       "      <td>Voting 5</td>\n",
       "      <td>Voting 6</td>\n",
       "      <td>Voting 7</td>\n",
       "      <td>Voting 8</td>\n",
       "      <td>Voting 9</td>\n",
       "      <td>Voting 10</td>\n",
       "      <td>Voting 11</td>\n",
       "      <td>Voting 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Score (TF)</th>\n",
       "      <td>0.973336</td>\n",
       "      <td>0.973609</td>\n",
       "      <td>0.965357</td>\n",
       "      <td>0.973677</td>\n",
       "      <td>0.974564</td>\n",
       "      <td>0.973472</td>\n",
       "      <td>0.967744</td>\n",
       "      <td>0.967676</td>\n",
       "      <td>0.967608</td>\n",
       "      <td>0.975041</td>\n",
       "      <td>0.96713</td>\n",
       "      <td>0.972245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (TF)</th>\n",
       "      <td>0.769839</td>\n",
       "      <td>0.771203</td>\n",
       "      <td>0.770112</td>\n",
       "      <td>0.767112</td>\n",
       "      <td>0.776929</td>\n",
       "      <td>0.769839</td>\n",
       "      <td>0.776111</td>\n",
       "      <td>0.775839</td>\n",
       "      <td>0.774475</td>\n",
       "      <td>0.775839</td>\n",
       "      <td>0.774475</td>\n",
       "      <td>0.773384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0         1         2         3         4         5   \\\n",
       "Model             Voting 1  Voting 2  Voting 3  Voting 4  Voting 5  Voting 6   \n",
       "Train Score (TF)  0.973336  0.973609  0.965357  0.973677  0.974564  0.973472   \n",
       "Test Score (TF)   0.769839  0.771203  0.770112  0.767112  0.776929  0.769839   \n",
       "\n",
       "                        6         7         8          9          10  \\\n",
       "Model             Voting 7  Voting 8  Voting 9  Voting 10  Voting 11   \n",
       "Train Score (TF)  0.967744  0.967676  0.967608   0.975041    0.96713   \n",
       "Test Score (TF)   0.776111  0.775839  0.774475   0.775839   0.774475   \n",
       "\n",
       "                         11  \n",
       "Model             Voting 12  \n",
       "Train Score (TF)   0.972245  \n",
       "Test Score (TF)    0.773384  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "data_labels_map = dict(enumerate(data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  3, 17, ...,  3,  1,  7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nBack in high school I worked as a lab assi...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nAE is in Dallas...try 214/241-6060 or 214/...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n[stuff deleted]\\n\\nOk, here's the solution t...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\n\\nYeah, it's the second one.  And I believ...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nIf a Christian means someone who believes in...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Target Label  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...            10   \n",
       "1  My brother is in the market for a high-perform...             3   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...            17   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...             3   \n",
       "4  1)    I have an old Jasmine drive which I cann...             4   \n",
       "5  \\n\\nBack in high school I worked as a lab assi...            12   \n",
       "6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...             4   \n",
       "7  \\n[stuff deleted]\\n\\nOk, here's the solution t...            10   \n",
       "8  \\n\\n\\nYeah, it's the second one.  And I believ...            10   \n",
       "9  \\nIf a Christian means someone who believes in...            19   \n",
       "\n",
       "                Target Name  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  \n",
       "5           sci.electronics  \n",
       "6     comp.sys.mac.hardware  \n",
       "7          rec.sport.hockey  \n",
       "8          rec.sport.hockey  \n",
       "9        talk.religion.misc  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus, target_labels, target_names = (data.data, data.target, [data_labels_map[label] for label in data.target])\n",
    "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels, 'Target Name': target_names})\n",
    "\n",
    "print(data_df.shape)\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18331, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df[~(data_df.Article.str.strip() == \"\")]\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (Best Combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_corpus = pp.normalize_corpus(corpus=data_df['Article'], html_stripping=True, contraction_expansion=True,\n",
    "                     text_lower_case=True, text_lemmatization=True, stopword_removal=True, accented_char_removal=True, \n",
    "                     special_char_removal=True)\n",
    "    \n",
    "data_df['Clean Article'] = norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18331"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10264,), (4400,), (3667,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names = train_test_split(np.array(data_df['Clean Article']),\n",
    "                                                                                                                         np.array(data_df['Target Label']),\n",
    "                                                                                                                         np.array(data_df['Target Name']),\n",
    "                                                                                                                         stratify=data_df['Target Label'],\n",
    "                                                                                                                         test_size=0.20, random_state=42)\n",
    "\n",
    "train_corpus, val_corpus, train_label_nums, val_label_nums, train_label_names, val_label_names = train_test_split(train_corpus,\n",
    "                                                                                                                          train_label_nums,\n",
    "                                                                                                                          train_label_names,\n",
    "                                                                                                                          stratify=train_label_nums,\n",
    "                                                                                                                          test_size=0.30, random_state=42)\n",
    "\n",
    "train_corpus.shape, val_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bag of Words Features with Classification Models (Unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)\n",
    "cv_train_features = cv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "cv_val_features = cv.transform(val_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (10264, 78525)  Valid features shape: (4400, 78525)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape,' Valid features shape:', cv_val_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8068978955572876\n",
      "Valid Accuracy: 0.6827272727272727\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(cv_train_features, train_label_names)\n",
    "\n",
    "mnb_bow_train_scores = mnb.score(cv_train_features, train_label_names)\n",
    "print('Train Accuracy:', mnb_bow_train_scores)\n",
    "\n",
    "mnb_bow_valid_scores = mnb.score(cv_val_features, val_label_names)\n",
    "print('Valid Accuracy:', mnb_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9943491816056118\n",
      "Valid Accuracy: 0.6843181818181818\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(cv_train_features, train_label_names)\n",
    "\n",
    "log_bow_train_scores = log.score(cv_train_features, train_label_names)\n",
    "print('Train Accuracy:', log_bow_train_scores)\n",
    "\n",
    "log_bow_valid_scores = log.score(cv_val_features, val_label_names)\n",
    "print('Valid Accuracy:', log_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9974668745128605\n",
      "Valid Accuracy: 0.6418181818181818\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(cv_train_features, train_label_names)\n",
    "\n",
    "svm_bow_train_scores = svm.score(cv_train_features, train_label_names)\n",
    "print('Train Accuracy:', svm_bow_train_scores)\n",
    "\n",
    "svm_bow_valid_scores = svm.score(cv_val_features, val_label_names)\n",
    "print('Valid Accuracy:', svm_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9711613406079501\n",
      "Valid Accuracy: 0.6377272727272727\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(cv_train_features, train_label_names)\n",
    "\n",
    "sgd_bow_train_scores = sgd.score(cv_train_features, train_label_names)\n",
    "print('Train Accuracy:', sgd_bow_train_scores)\n",
    "\n",
    "sgd_bow_valid_scores = sgd.score(cv_val_features, val_label_names)\n",
    "print('Valid Accuracy:', sgd_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9982462977396727\n",
      "Valid Accuracy: 0.6636363636363637\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(cv_train_features, train_label_names)\n",
    "\n",
    "rfc_bow_train_scores = rfc.score(cv_train_features, train_label_names)\n",
    "print('Train Accuracy:', rfc_bow_train_scores)\n",
    "\n",
    "rfc_bow_valid_scores = rfc.score(cv_val_features, val_label_names)\n",
    "print('Valid Accuracy:', rfc_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9970771628994544\n",
      "Valid Accuracy: 0.7127272727272728\n"
     ]
    }
   ],
   "source": [
    "voting = VotingClassifier(estimators=[('mnb', mnb), ('log', log), ('svm', svm), ('sgd', sgd), ('rfc', rfc)], voting='hard')\n",
    "voting_fitted = voting.fit(cv_train_features, train_label_names)\n",
    "\n",
    "voting_bow_train_scores = voting.score(cv_train_features, train_label_names)\n",
    "print('Train Accuracy:', voting_bow_train_scores)\n",
    "\n",
    "voting_bow_val_scores = voting.score(cv_val_features, val_label_names)\n",
    "print('Valid Accuracy:', voting_bow_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bag of Words Features with Classification Models (Unigrams, Bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_bi = CountVectorizer(ngram_range=(1,2), binary=False, min_df=0.0, max_df=1.0)\n",
    "cv_bi_train_features = cv_bi.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "cv_bi_val_features = cv_bi.transform(val_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (10264, 782403)  Valid features shape: (4400, 782403)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_bi_train_features.shape,' Valid features shape:', cv_bi_val_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9623928293063133\n",
      "Valid Accuracy: 0.6690909090909091\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(cv_bi_train_features, train_label_names)\n",
    "\n",
    "mnb_bi_bow_train_scores = mnb.score(cv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', mnb_bi_bow_train_scores)\n",
    "\n",
    "mnb_bi_bow_valid_scores = mnb.score(cv_bi_val_features, val_label_names)\n",
    "print('Valid Accuracy:', mnb_bi_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9967848791893998\n",
      "Valid Accuracy: 0.6929545454545455\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(cv_bi_train_features, train_label_names)\n",
    "\n",
    "log_bi_bow_train_scores = log.score(cv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', log_bi_bow_train_scores)\n",
    "\n",
    "log_bi_bow_valid_scores = log.score(cv_bi_val_features, val_label_names)\n",
    "print('Valid Accuracy:', log_bi_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9982462977396727\n",
      "Valid Accuracy: 0.6695454545454546\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(cv_bi_train_features, train_label_names)\n",
    "\n",
    "svm_bi_bow_train_scores = svm.score(cv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', svm_bi_bow_train_scores)\n",
    "\n",
    "svm_bi_bow_valid_scores = svm.score(cv_bi_val_features, val_label_names)\n",
    "print('Valid Accuracy:', svm_bi_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9916212003117693\n",
      "Valid Accuracy: 0.6584090909090909\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(cv_bi_train_features, train_label_names)\n",
    "\n",
    "sgd_bi_bow_train_scores = sgd.score(cv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', sgd_bi_bow_train_scores)\n",
    "\n",
    "sgd_bi_bow_valid_scores = sgd.score(cv_bi_val_features, val_label_names)\n",
    "print('Valid Accuracy:', sgd_bi_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9982462977396727\n",
      "Valid Accuracy: 0.6586363636363637\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(cv_bi_train_features, train_label_names)\n",
    "\n",
    "rfc_bi_bow_train_scores = rfc.score(cv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', rfc_bi_bow_train_scores)\n",
    "\n",
    "rfc_bi_bow_valid_scores = rfc.score(cv_bi_val_features, val_label_names)\n",
    "print('Valid Accuracy:', rfc_bi_bow_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.997564302416212\n",
      "Valid Accuracy: 0.7206818181818182\n"
     ]
    }
   ],
   "source": [
    "voting = VotingClassifier(estimators=[('mnb', mnb), ('log', log), ('svm', svm), ('sgd', sgd), ('rfc', rfc)], voting='hard')\n",
    "voting_fitted = voting.fit(cv_bi_train_features, train_label_names)\n",
    "\n",
    "voting_bi_bow_train_scores = voting.score(cv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', voting_bi_bow_train_scores)\n",
    "\n",
    "voting_bi_bow_val_scores = voting.score(cv_bi_val_features, val_label_names)\n",
    "print('Valid Accuracy:', voting_bi_bow_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 TF-IDF Features with Classification Models (Unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF model:> Train features shape: (10264, 78525)  Valid features shape: (4400, 78525)\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0)\n",
    "tv_train_features = tv.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "tv_valid_features = tv.transform(val_corpus)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape,' Valid features shape:', tv_valid_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8685697583787997\n",
      "Valid Accuracy: 0.7281818181818182\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(tv_train_features, train_label_names)\n",
    "\n",
    "mnb_tfidf_train_scores = mnb.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', mnb_tfidf_train_scores)\n",
    "\n",
    "mnb_tfidf_valid_scores = mnb.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', mnb_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.92371395167576\n",
      "Valid Accuracy: 0.7481818181818182\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(tv_train_features, train_label_names)\n",
    "\n",
    "log_tfidf_train_scores = log.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', log_tfidf_train_scores)\n",
    "\n",
    "log_tfidf_valid_scores = log.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', log_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9952260327357755\n",
      "Valid Accuracy: 0.7654545454545455\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, train_label_names)\n",
    "\n",
    "svm_tfidf_train_scores = svm.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', svm_tfidf_train_scores)\n",
    "\n",
    "svm_tfidf_valid_scores = svm.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', svm_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9761301636788776\n",
      "Valid Accuracy: 0.7670454545454546\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(tv_train_features, train_label_names)\n",
    "\n",
    "sgd_tfidf_train_scores = sgd.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', sgd_tfidf_train_scores)\n",
    "\n",
    "sgd_tfidf_valid_scores = sgd.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', sgd_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9982462977396727\n",
      "Valid Accuracy: 0.6740909090909091\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(tv_train_features, train_label_names)\n",
    "\n",
    "rfc_tfidf_train_scores = rfc.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', rfc_tfidf_train_scores)\n",
    "\n",
    "rfc_tfidf_valid_scores = rfc.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', rfc_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9796375681995323\n",
      "Valid Accuracy: 0.7361363636363636\n"
     ]
    }
   ],
   "source": [
    "voting = VotingClassifier(estimators=[('mnb', mnb), ('log', log), ('svm', svm), ('sgd', sgd), ('rfc', rfc)], voting='hard')\n",
    "voting_fitted = voting.fit(tv_train_features, train_label_names)\n",
    "\n",
    "voting_tfidf_train_scores = voting.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', voting_tfidf_train_scores)\n",
    "\n",
    "voting_tfidf_val_scores = voting.score(cv_val_features, val_label_names)\n",
    "print('Valid Accuracy:', voting_tfidf_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 TF-IDF Features with Classification Models (Unigrams and Bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF model:> Train features shape: (10264, 782403)  Valid features shape: (4400, 782403)\n"
     ]
    }
   ],
   "source": [
    "tv_bi = TfidfVectorizer(ngram_range=(1,2), use_idf=True, min_df=0.0, max_df=1.0)\n",
    "tv_bi_train_features = tv_bi.fit_transform(train_corpus)\n",
    "\n",
    "# transform test articles into features\n",
    "tv_bi_valid_features = tv_bi.transform(val_corpus)\n",
    "print('TFIDF model:> Train features shape:', tv_bi_train_features.shape,' Valid features shape:', tv_bi_valid_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.953526890101325\n",
      "Valid Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(tv_bi_train_features, train_label_names)\n",
    "\n",
    "mnb_bi_tfidf_train_scores = mnb.score(tv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', mnb_bi_tfidf_train_scores)\n",
    "\n",
    "mnb_bi_tfidf_valid_scores = mnb.score(tv_bi_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', mnb_bi_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9674590802805924\n",
      "Valid Accuracy: 0.7481818181818182\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(tv_bi_train_features, train_label_names)\n",
    "\n",
    "log_bi_tfidf_train_scores = log.score(tv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', log_bi_tfidf_train_scores)\n",
    "\n",
    "log_bi_tfidf_valid_scores = log.score(tv_bi_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', log_bi_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9974668745128605\n",
      "Valid Accuracy: 0.7793181818181818\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_bi_train_features, train_label_names)\n",
    "\n",
    "svm_bi_tfidf_train_scores = svm.score(tv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', svm_bi_tfidf_train_scores)\n",
    "\n",
    "svm_bi_tfidf_valid_scores = svm.score(tv_bi_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', svm_bi_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9942517537022604\n",
      "Valid Accuracy: 0.7797727272727273\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(tv_bi_train_features, train_label_names)\n",
    "\n",
    "sgd_bi_tfidf_train_scores = sgd.score(tv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', sgd_bi_tfidf_train_scores)\n",
    "\n",
    "sgd_bi_tfidf_valid_scores = sgd.score(tv_bi_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', sgd_bi_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9982462977396727\n",
      "Valid Accuracy: 0.6704545454545454\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(tv_bi_train_features, train_label_names)\n",
    "\n",
    "rfc_bi_tfidf_train_scores = rfc.score(tv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', rfc_bi_tfidf_train_scores)\n",
    "\n",
    "rfc_bi_tfidf_valid_scores = rfc.score(tv_bi_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', rfc_bi_tfidf_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9943491816056118\n",
      "Valid Accuracy: 0.7718181818181818\n"
     ]
    }
   ],
   "source": [
    "voting = VotingClassifier(estimators=[('mnb', mnb), ('log', log), ('svm', svm), ('sgd', sgd), ('rfc', rfc)], voting='hard')\n",
    "voting_fitted = voting.fit(tv_bi_train_features, train_label_names)\n",
    "\n",
    "voting_bi_tfidf_train_scores = voting.score(tv_bi_train_features, train_label_names)\n",
    "print('Train Accuracy:', voting_bi_tfidf_train_scores)\n",
    "\n",
    "voting_bi_tfidf_val_scores = voting.score(tv_bi_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', voting_bi_tfidf_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BOW + TF-IDF Transformer  with Classification Models (Unigrams and Bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9944466095089634\n",
      "Valid Accuracy: 0.7709090909090909\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "log = LogisticRegression()\n",
    "svm = LinearSVC(penalty='l2', C=1)\n",
    "sgd = SGDClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "voting_bow_tfidf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2), binary=False, min_df=0.0, max_df=1.0)),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('voting', VotingClassifier(estimators=[('mnb', mnb), ('log', log), ('svm', svm), ('sgd', sgd), ('rfc', rfc)], voting='hard')),])\n",
    "\n",
    "voting_bow_tfidf.fit_transform(train_corpus, train_label_names)\n",
    "\n",
    "voting_bow_tfidf_train_score = voting_bow_tfidf.score(train_corpus, train_label_names)\n",
    "print('Train Accuracy:', voting_bow_tfidf_train_score)\n",
    "\n",
    "voting_bow_tfidf_valid_score = voting_bow_tfidf.score(val_corpus, val_label_names)\n",
    "print('Valid Accuracy:', voting_bow_tfidf_valid_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vec (Word Embedding) with Classification Models using GLOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following file should be dowloaded for the Word2Vec code to work: \n",
    "https://www.kaggle.com/danielwillgeorge/glove6b100dtxt?select=glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_data():\n",
    "    # read the word to vec file\n",
    "    GLOVE_6B_100D_PATH = \"glove.6B.100d.txt\"\n",
    "    dim = 100\n",
    "    glove_small = {}\n",
    "    with open(GLOVE_6B_100D_PATH, \"rb\") as infile:\n",
    "        for line in infile:\n",
    "            parts = line.split()\n",
    "            try:\n",
    "                word = parts[0].decode(\"utf-8\")\n",
    "                x = []\n",
    "                for i in range(len(parts)-1):\n",
    "                    x.append(float(parts[i+1].decode(\"utf-8\")))\n",
    "                glove_small[word] = x\n",
    "            except: \n",
    "                print('')\n",
    "    return glove_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_transform(dataset, word2vec, dim):\n",
    "    trans_data = []\n",
    "    for doc in dataset:\n",
    "        words = doc.lower().split()\n",
    "        w_length = 1\n",
    "        data = np.zeros(dim)\n",
    "        for i in range(len(words)):\n",
    "            if words[i] in word2vec and words[i] not in stop_words:\n",
    "                data = data + word2vec[words[i]]\n",
    "                w_length = w_length + 1\n",
    "        data = data / float(w_length)\n",
    "        trans_data.append(data)\n",
    "    return trans_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = load_glove_data()\n",
    "\n",
    "train_docs = word2vec_transform(train_corpus, word2vec, 100)\n",
    "valid_docs = word2vec_transform(val_corpus, word2vec, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.6515003897116134\n",
      "Valid Accuracy: 0.6013636363636363\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(train_docs, train_label_names)\n",
    "\n",
    "log_word_train_scores = log.score(train_docs, train_label_names)\n",
    "print('Train Accuracy:', log_word_train_scores)\n",
    "\n",
    "log_word_valid_scores = log.score(valid_docs, val_label_names)\n",
    "print('Valid Accuracy:', log_word_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.6403936087295401\n",
      "Valid Accuracy: 0.5984090909090909\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(train_docs, train_label_names)\n",
    "\n",
    "svm_word_train_scores = svm.score(train_docs, train_label_names)\n",
    "print('Train Accuracy:', svm_word_train_scores)\n",
    "\n",
    "svm_word_valid_scores = svm.score(valid_docs, val_label_names)\n",
    "print('Valid Accuracy:', svm_word_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.565958690568979\n",
      "Valid Accuracy: 0.5295454545454545\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(train_docs, train_label_names)\n",
    "\n",
    "sgd_word_train_scores = sgd.score(train_docs, train_label_names)\n",
    "print('Train Accuracy:', sgd_word_train_scores)\n",
    "\n",
    "sgd_word_valid_scores = sgd.score(valid_docs, val_label_names)\n",
    "print('Valid Accuracy:', sgd_word_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9983437256430242\n",
      "Valid Accuracy: 0.5436363636363636\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(train_docs, train_label_names)\n",
    "\n",
    "rfc_word_train_scores = rfc.score(train_docs, train_label_names)\n",
    "print('Train Accuracy:', rfc_word_train_scores)\n",
    "\n",
    "rfc_word_valid_scores = rfc.score(valid_docs, val_label_names)\n",
    "print('Valid Accuracy:', rfc_word_valid_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.6844310210444271\n",
      "Valid Accuracy: 0.6018181818181818\n"
     ]
    }
   ],
   "source": [
    "voting = VotingClassifier(estimators=[('log', log), ('svm', svm), ('sgd', sgd), ('rfc', rfc)], voting='hard')\n",
    "voting_fitted = voting.fit(train_docs, train_label_names)\n",
    "\n",
    "voting_word_train_scores = voting.score(train_docs, train_label_names)\n",
    "print('Train Accuracy:', voting_word_train_scores)\n",
    "\n",
    "voting_word_val_scores = voting.score(valid_docs, val_label_names)\n",
    "print('Valid Accuracy:', voting_word_val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MNB</th>\n",
       "      <th>LOG</th>\n",
       "      <th>SVM</th>\n",
       "      <th>SGD</th>\n",
       "      <th>RFC</th>\n",
       "      <th>Voting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BOW (Unigram)</td>\n",
       "      <td>0.682727</td>\n",
       "      <td>0.684318</td>\n",
       "      <td>0.641818</td>\n",
       "      <td>0.637727</td>\n",
       "      <td>0.663636</td>\n",
       "      <td>0.712727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BOW (Uni-Bigram)</td>\n",
       "      <td>0.669091</td>\n",
       "      <td>0.692955</td>\n",
       "      <td>0.669545</td>\n",
       "      <td>0.658409</td>\n",
       "      <td>0.658636</td>\n",
       "      <td>0.720682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TF-IDF (Unigram)</td>\n",
       "      <td>0.728182</td>\n",
       "      <td>0.748182</td>\n",
       "      <td>0.765455</td>\n",
       "      <td>0.767045</td>\n",
       "      <td>0.674091</td>\n",
       "      <td>0.736136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TF-IDF (Uni-Bigram)</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.748182</td>\n",
       "      <td>0.779318</td>\n",
       "      <td>0.779773</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.771818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BOW + TF-IDF</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.770909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Word2Vec-GLOVE</td>\n",
       "      <td></td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.598409</td>\n",
       "      <td>0.529545</td>\n",
       "      <td>0.543636</td>\n",
       "      <td>0.601818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model       MNB       LOG       SVM       SGD       RFC  \\\n",
       "0        BOW (Unigram)  0.682727  0.684318  0.641818  0.637727  0.663636   \n",
       "1     BOW (Uni-Bigram)  0.669091  0.692955  0.669545  0.658409  0.658636   \n",
       "2     TF-IDF (Unigram)  0.728182  0.748182  0.765455  0.767045  0.674091   \n",
       "3  TF-IDF (Uni-Bigram)      0.72  0.748182  0.779318  0.779773  0.670455   \n",
       "4         BOW + TF-IDF                                                     \n",
       "5       Word2Vec-GLOVE            0.601364  0.598409  0.529545  0.543636   \n",
       "\n",
       "     Voting  \n",
       "0  0.712727  \n",
       "1  0.720682  \n",
       "2  0.736136  \n",
       "3  0.771818  \n",
       "4  0.770909  \n",
       "5  0.601818  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_scores = pd.DataFrame([['BOW (Unigram)', mnb_bow_valid_scores, log_bow_valid_scores, svm_bow_valid_scores, sgd_bow_valid_scores, rfc_bow_valid_scores, voting_bow_val_scores],\n",
    "          ['BOW (Uni-Bigram)', mnb_bi_bow_valid_scores, log_bi_bow_valid_scores, svm_bi_bow_valid_scores, sgd_bi_bow_valid_scores, rfc_bi_bow_valid_scores, voting_bi_bow_val_scores],\n",
    "          ['TF-IDF (Unigram)', mnb_tfidf_valid_scores, log_tfidf_valid_scores, svm_tfidf_valid_scores, sgd_tfidf_valid_scores, rfc_tfidf_valid_scores, voting_tfidf_val_scores],\n",
    "          ['TF-IDF (Uni-Bigram)', mnb_bi_tfidf_valid_scores, log_bi_tfidf_valid_scores, svm_bi_tfidf_valid_scores, sgd_bi_tfidf_valid_scores, rfc_bi_tfidf_valid_scores, voting_bi_tfidf_val_scores],\n",
    "          ['BOW + TF-IDF', '', '', '', '', '', voting_bow_tfidf_valid_score],\n",
    "          ['Word2Vec-GLOVE', '', log_word_valid_scores, svm_word_valid_scores, sgd_word_valid_scores, rfc_word_valid_scores, voting_word_val_scores]],\n",
    "          columns = ['Model', 'MNB', 'LOG', 'SVM', 'SGD', 'RFC', 'Voting'])\n",
    "\n",
    "fe_scores.to_pickle(\"fe_scores.pkl\")\n",
    "\n",
    "fe_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TFIDF (Unigram and Bigram) seems to have the best accuracy scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For a better flow of work, i will recreate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF model:> Train features shape: (10264, 782403)  Valid features shape: (4400, 782403) Test features shape: (3667, 782403)\n"
     ]
    }
   ],
   "source": [
    "tv = TfidfVectorizer(ngram_range=(1,2), use_idf=True, min_df=0.0, max_df=1.0)\n",
    "\n",
    "tv_train_features = tv.fit_transform(train_corpus)\n",
    "\n",
    "tv_valid_features = tv.transform(val_corpus)\n",
    "\n",
    "tv_test_features = tv.transform(test_corpus)\n",
    "\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape,' Valid features shape:', tv_valid_features.shape, 'Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 'refit = True' saves the best model to the GridSearch object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.772701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011837</td>\n",
       "      <td>0.773967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013673</td>\n",
       "      <td>0.772798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015510</td>\n",
       "      <td>0.772408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.017347</td>\n",
       "      <td>0.771824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.019184</td>\n",
       "      <td>0.771337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.021020</td>\n",
       "      <td>0.770557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.022857</td>\n",
       "      <td>0.770167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.024694</td>\n",
       "      <td>0.769388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.026531</td>\n",
       "      <td>0.768999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.028367</td>\n",
       "      <td>0.768999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.030204</td>\n",
       "      <td>0.768706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.032041</td>\n",
       "      <td>0.767830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.033878</td>\n",
       "      <td>0.767148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.767342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.037551</td>\n",
       "      <td>0.767148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.039388</td>\n",
       "      <td>0.766466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.041224</td>\n",
       "      <td>0.765881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.043061</td>\n",
       "      <td>0.765199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.044898</td>\n",
       "      <td>0.765199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.046735</td>\n",
       "      <td>0.764712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.048571</td>\n",
       "      <td>0.763738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.050408</td>\n",
       "      <td>0.763445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.052245</td>\n",
       "      <td>0.762568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.054082</td>\n",
       "      <td>0.762276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.055918</td>\n",
       "      <td>0.761886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.057755</td>\n",
       "      <td>0.761302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.059592</td>\n",
       "      <td>0.761009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.061429</td>\n",
       "      <td>0.760522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.063265</td>\n",
       "      <td>0.760327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.065102</td>\n",
       "      <td>0.759937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.066939</td>\n",
       "      <td>0.760132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.068776</td>\n",
       "      <td>0.759840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.070612</td>\n",
       "      <td>0.759353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.072449</td>\n",
       "      <td>0.758866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.074286</td>\n",
       "      <td>0.758281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.076122</td>\n",
       "      <td>0.757891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.077959</td>\n",
       "      <td>0.757696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.079796</td>\n",
       "      <td>0.757404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.757112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.083469</td>\n",
       "      <td>0.757014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.085306</td>\n",
       "      <td>0.756625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.087143</td>\n",
       "      <td>0.756527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.088980</td>\n",
       "      <td>0.755748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.090816</td>\n",
       "      <td>0.755650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.092653</td>\n",
       "      <td>0.755456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.094490</td>\n",
       "      <td>0.755261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.096327</td>\n",
       "      <td>0.754676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.098163</td>\n",
       "      <td>0.754286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.754092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alpha  Accuracy\n",
       "0   0.010000  0.772701\n",
       "1   0.011837  0.773967\n",
       "2   0.013673  0.772798\n",
       "3   0.015510  0.772408\n",
       "4   0.017347  0.771824\n",
       "5   0.019184  0.771337\n",
       "6   0.021020  0.770557\n",
       "7   0.022857  0.770167\n",
       "8   0.024694  0.769388\n",
       "9   0.026531  0.768999\n",
       "10  0.028367  0.768999\n",
       "11  0.030204  0.768706\n",
       "12  0.032041  0.767830\n",
       "13  0.033878  0.767148\n",
       "14  0.035714  0.767342\n",
       "15  0.037551  0.767148\n",
       "16  0.039388  0.766466\n",
       "17  0.041224  0.765881\n",
       "18  0.043061  0.765199\n",
       "19  0.044898  0.765199\n",
       "20  0.046735  0.764712\n",
       "21  0.048571  0.763738\n",
       "22  0.050408  0.763445\n",
       "23  0.052245  0.762568\n",
       "24  0.054082  0.762276\n",
       "25  0.055918  0.761886\n",
       "26  0.057755  0.761302\n",
       "27  0.059592  0.761009\n",
       "28  0.061429  0.760522\n",
       "29  0.063265  0.760327\n",
       "30  0.065102  0.759937\n",
       "31  0.066939  0.760132\n",
       "32  0.068776  0.759840\n",
       "33  0.070612  0.759353\n",
       "34  0.072449  0.758866\n",
       "35  0.074286  0.758281\n",
       "36  0.076122  0.757891\n",
       "37  0.077959  0.757696\n",
       "38  0.079796  0.757404\n",
       "39  0.081633  0.757112\n",
       "40  0.083469  0.757014\n",
       "41  0.085306  0.756625\n",
       "42  0.087143  0.756527\n",
       "43  0.088980  0.755748\n",
       "44  0.090816  0.755650\n",
       "45  0.092653  0.755456\n",
       "46  0.094490  0.755261\n",
       "47  0.096327  0.754676\n",
       "48  0.098163  0.754286\n",
       "49  0.100000  0.754092"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_mnb = {'alpha': np.linspace(0.01,0.1,50)} \n",
    "\n",
    "random_mnb_class = RandomizedSearchCV(\n",
    "    estimator = MultinomialNB(),\n",
    "    param_distributions = param_grid_mnb,\n",
    "    n_iter = 50,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "random_mnb_class.fit(tv_train_features, train_label_names)\n",
    "\n",
    "df_mnb = pd.concat([pd.DataFrame(random_mnb_class.cv_results_[\"params\"]),pd.DataFrame(random_mnb_class.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n",
    "df_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9964925954793453\n",
      "Valid Accuracy: 0.7818181818181819\n"
     ]
    }
   ],
   "source": [
    "random_mnb_class_train_scores = random_mnb_class.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', random_mnb_class_train_scores)\n",
    "\n",
    "random_mnb_class_val_scores = random_mnb_class.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', random_mnb_class_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complement Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.737822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.739771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.740453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.741427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.743083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.744058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.745422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.746396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.747760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.748637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.749416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.749709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.750585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.751267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.751657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.752242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.003939</td>\n",
       "      <td>0.752826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.004122</td>\n",
       "      <td>0.753021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.004306</td>\n",
       "      <td>0.753411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.753703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.753898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.754190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.005041</td>\n",
       "      <td>0.754775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.005224</td>\n",
       "      <td>0.755846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.756431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005592</td>\n",
       "      <td>0.756821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005776</td>\n",
       "      <td>0.757308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.757600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.006143</td>\n",
       "      <td>0.757990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.006327</td>\n",
       "      <td>0.757795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.006510</td>\n",
       "      <td>0.757892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.006694</td>\n",
       "      <td>0.758282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.006878</td>\n",
       "      <td>0.758769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.758867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.007245</td>\n",
       "      <td>0.758964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.007429</td>\n",
       "      <td>0.759354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.007612</td>\n",
       "      <td>0.759451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.007796</td>\n",
       "      <td>0.759549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.007980</td>\n",
       "      <td>0.759938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.759938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.760133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.008531</td>\n",
       "      <td>0.760133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.008714</td>\n",
       "      <td>0.760328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.008898</td>\n",
       "      <td>0.760718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.009082</td>\n",
       "      <td>0.760815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.009265</td>\n",
       "      <td>0.761205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.009449</td>\n",
       "      <td>0.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.009633</td>\n",
       "      <td>0.761595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.009816</td>\n",
       "      <td>0.761984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.762179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alpha  Accuracy\n",
       "0   0.001000  0.737822\n",
       "1   0.001184  0.739771\n",
       "2   0.001367  0.740453\n",
       "3   0.001551  0.741427\n",
       "4   0.001735  0.743083\n",
       "5   0.001918  0.744058\n",
       "6   0.002102  0.745422\n",
       "7   0.002286  0.746396\n",
       "8   0.002469  0.747760\n",
       "9   0.002653  0.748637\n",
       "10  0.002837  0.749416\n",
       "11  0.003020  0.749709\n",
       "12  0.003204  0.750585\n",
       "13  0.003388  0.751267\n",
       "14  0.003571  0.751657\n",
       "15  0.003755  0.752242\n",
       "16  0.003939  0.752826\n",
       "17  0.004122  0.753021\n",
       "18  0.004306  0.753411\n",
       "19  0.004490  0.753703\n",
       "20  0.004673  0.753898\n",
       "21  0.004857  0.754190\n",
       "22  0.005041  0.754775\n",
       "23  0.005224  0.755846\n",
       "24  0.005408  0.756431\n",
       "25  0.005592  0.756821\n",
       "26  0.005776  0.757308\n",
       "27  0.005959  0.757600\n",
       "28  0.006143  0.757990\n",
       "29  0.006327  0.757795\n",
       "30  0.006510  0.757892\n",
       "31  0.006694  0.758282\n",
       "32  0.006878  0.758769\n",
       "33  0.007061  0.758867\n",
       "34  0.007245  0.758964\n",
       "35  0.007429  0.759354\n",
       "36  0.007612  0.759451\n",
       "37  0.007796  0.759549\n",
       "38  0.007980  0.759938\n",
       "39  0.008163  0.759938\n",
       "40  0.008347  0.760133\n",
       "41  0.008531  0.760133\n",
       "42  0.008714  0.760328\n",
       "43  0.008898  0.760718\n",
       "44  0.009082  0.760815\n",
       "45  0.009265  0.761205\n",
       "46  0.009449  0.761400\n",
       "47  0.009633  0.761595\n",
       "48  0.009816  0.761984\n",
       "49  0.010000  0.762179"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_cnb = {'alpha': np.linspace(0.001,0.01,50)} \n",
    "\n",
    "random_cnb_class = RandomizedSearchCV(\n",
    "    estimator = ComplementNB(),\n",
    "    param_distributions = param_grid_cnb,\n",
    "    n_iter = 50,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "random_cnb_class.fit(tv_train_features, train_label_names)\n",
    "\n",
    "df_cnb = pd.concat([pd.DataFrame(random_cnb_class.cv_results_[\"params\"]),pd.DataFrame(random_cnb_class.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n",
    "df_cnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9965900233826968\n",
      "Valid Accuracy: 0.7706818181818181\n"
     ]
    }
   ],
   "source": [
    "random_cnb_class_train_scores = random_cnb_class.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', random_cnb_class_train_scores)\n",
    "\n",
    "random_cnb_class_val_scores = random_cnb_class.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', random_cnb_class_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>solver</th>\n",
       "      <th>penalty</th>\n",
       "      <th>C</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>liblinear</td>\n",
       "      <td>none</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>l2</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>none</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>liblinear</td>\n",
       "      <td>l2</td>\n",
       "      <td>100</td>\n",
       "      <td>0.755261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>l2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.750194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>liblinear</td>\n",
       "      <td>none</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>liblinear</td>\n",
       "      <td>l2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.751168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>liblinear</td>\n",
       "      <td>l1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.702942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lbfgs</td>\n",
       "      <td>l1</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>liblinear</td>\n",
       "      <td>l1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.700798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      solver penalty    C  Accuracy\n",
       "0  liblinear    none   50       NaN\n",
       "1      lbfgs      l2  100       NaN\n",
       "2      lbfgs    none  100       NaN\n",
       "3  liblinear      l2  100  0.755261\n",
       "4      lbfgs      l2   10  0.750194\n",
       "5  liblinear    none   10       NaN\n",
       "6  liblinear      l2   10  0.751168\n",
       "7  liblinear      l1  100  0.702942\n",
       "8      lbfgs      l1   10       NaN\n",
       "9  liblinear      l1   50  0.700798"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_log = {'solver': ['lbfgs', 'liblinear'], 'penalty': ['none', 'l1', 'l2'], 'C': [100, 50, 10]} \n",
    "\n",
    "random_log_class = RandomizedSearchCV(\n",
    "    estimator = LogisticRegression(),\n",
    "    param_distributions = param_grid_log,\n",
    "    n_iter = 10,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "random_log_class.fit(tv_train_features, train_label_names)\n",
    "\n",
    "df_log = pd.concat([pd.DataFrame(random_log_class.cv_results_[\"params\"]),pd.DataFrame(random_log_class.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n",
    "df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9981488698363211\n",
      "Valid Accuracy: 0.7763636363636364\n"
     ]
    }
   ],
   "source": [
    "random_log_class_train_scores = random_log_class.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', random_log_class_train_scores)\n",
    "\n",
    "random_log_class_val_scores = random_log_class.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', random_log_class_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel</th>\n",
       "      <th>C</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rbf</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.721550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rbf</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kernel     C  Accuracy\n",
       "0  linear  10.0  0.742400\n",
       "1     rbf  10.0  0.721550\n",
       "2  linear   1.0  0.740938\n",
       "3     rbf   1.0  0.694368"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_svm = {'kernel': ['linear', 'rbf'], 'C': [10, 1.0]} \n",
    "\n",
    "random_svm_class = RandomizedSearchCV(\n",
    "    estimator = SVC(),\n",
    "    param_distributions = param_grid_svm,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "random_svm_class.fit(tv_train_features, train_label_names)\n",
    "\n",
    "df_svm = pd.concat([pd.DataFrame(random_svm_class.cv_results_[\"params\"]),pd.DataFrame(random_svm_class.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n",
    "df_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.997954014029618\n",
      "Valid Accuracy: 0.7672727272727272\n"
     ]
    }
   ],
   "source": [
    "random_svm_class_train_scores = random_svm_class.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', random_svm_class_train_scores)\n",
    "\n",
    "random_svm_class_val_scores = random_svm_class.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', random_svm_class_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>penalty</th>\n",
       "      <th>alpha</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.720088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.715218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.753214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.749220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.760522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.733047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.735970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.570537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.735970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>elasticnet</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.124511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      penalty     alpha  Accuracy\n",
       "0          l2  0.000001  0.720088\n",
       "1  elasticnet  0.000001  0.715218\n",
       "2          l2  0.000010  0.753214\n",
       "3  elasticnet  0.000010  0.749220\n",
       "4          l2  0.000100  0.760522\n",
       "5  elasticnet  0.000100  0.733047\n",
       "6          l2  0.001000  0.735970\n",
       "7  elasticnet  0.001000  0.570537\n",
       "8          l2  0.010000  0.735970\n",
       "9  elasticnet  0.010000  0.124511"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_sgd = {'penalty': ['l2', 'elasticnet'], 'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01]} \n",
    "\n",
    "random_sgd_class = RandomizedSearchCV(\n",
    "    estimator = SGDClassifier(),\n",
    "    param_distributions = param_grid_sgd,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "random_sgd_class.fit(tv_train_features, train_label_names)\n",
    "\n",
    "df_sgd = pd.concat([pd.DataFrame(random_sgd_class.cv_results_[\"params\"]),pd.DataFrame(random_sgd_class.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n",
    "df_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9942517537022604\n",
      "Valid Accuracy: 0.7804545454545454\n"
     ]
    }
   ],
   "source": [
    "random_sgd_class_train_scores = random_sgd_class.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', random_sgd_class_train_scores)\n",
    "\n",
    "random_sgd_class_val_scores = random_sgd_class.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', random_sgd_class_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>criterion</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.423617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.597428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.626559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.644291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.277572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.457229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.529910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.565277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators criterion  Accuracy\n",
       "0            10      gini  0.423617\n",
       "1            50      gini  0.597428\n",
       "2           100      gini  0.626559\n",
       "3           200      gini  0.644291\n",
       "4            10   entropy  0.277572\n",
       "5            50   entropy  0.457229\n",
       "6           100   entropy  0.529910\n",
       "7           200   entropy  0.565277"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_rfc = {'n_estimators': [10, 50, 100, 200], 'criterion': ['gini', 'entropy']} \n",
    "\n",
    "random_rfc_class = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(),\n",
    "    param_distributions = param_grid_rfc,\n",
    "    scoring='accuracy', n_jobs=4, cv = 2, refit=True, return_train_score = True)\n",
    "\n",
    "random_rfc_class.fit(tv_train_features, train_label_names)\n",
    "\n",
    "df_rfc = pd.concat([pd.DataFrame(random_rfc_class.cv_results_[\"params\"]),pd.DataFrame(random_rfc_class.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n",
    "df_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9982462977396727\n",
      "Valid Accuracy: 0.6852272727272727\n"
     ]
    }
   ],
   "source": [
    "random_rfc_class_train_scores = random_rfc_class.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', random_rfc_class_train_scores)\n",
    "\n",
    "random_rfc_class_val_scores = random_rfc_class.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', random_rfc_class_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors Classifier (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_neighbors</th>\n",
       "      <th>metric</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.660366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.657151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.652767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>euclidean</td>\n",
       "      <td>0.647214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.054462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>120</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.058262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.059918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200</td>\n",
       "      <td>manhattan</td>\n",
       "      <td>0.053780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_neighbors     metric  Accuracy\n",
       "0          100  euclidean  0.660366\n",
       "1          120  euclidean  0.657151\n",
       "2          150  euclidean  0.652767\n",
       "3          200  euclidean  0.647214\n",
       "4          100  manhattan  0.054462\n",
       "5          120  manhattan  0.058262\n",
       "6          150  manhattan  0.059918\n",
       "7          200  manhattan  0.053780"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid_knn = {'n_neighbors': [100, 120, 150, 200], 'metric': ['euclidean', 'manhattan']} \n",
    "\n",
    "random_knn_class = RandomizedSearchCV(\n",
    "    estimator = KNeighborsClassifier(),\n",
    "    param_distributions = param_grid_knn,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "random_knn_class.fit(tv_train_features, train_label_names)\n",
    "\n",
    "df_knn = pd.concat([pd.DataFrame(random_knn_class.cv_results_[\"params\"]),pd.DataFrame(random_knn_class.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n",
    "df_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.6976812159002338\n",
      "Valid Accuracy: 0.6765909090909091\n"
     ]
    }
   ],
   "source": [
    "random_knn_class_train_scores = random_knn_class.score(tv_train_features, train_label_names)\n",
    "print('Train Accuracy:', random_knn_class_train_scores)\n",
    "\n",
    "random_knn_class_val_scores = random_knn_class.score(tv_valid_features, val_label_names)\n",
    "print('Valid Accuracy:', random_knn_class_val_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Models on the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7763839650940824\n"
     ]
    }
   ],
   "source": [
    "random_mnb_class_test_scores = random_mnb_class.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', random_mnb_class_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7657485683119717\n"
     ]
    }
   ],
   "source": [
    "random_cnb_class_test_scores = random_cnb_class.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', random_cnb_class_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7725661303517862\n"
     ]
    }
   ],
   "source": [
    "random_log_class_test_scores = random_log_class.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', random_log_class_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_svm_class_test_scores = random_svm_class.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', random_svm_class_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7722934278701936\n"
     ]
    }
   ],
   "source": [
    "random_sgd_class_test_scores = random_sgd_class.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', random_sgd_class_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.676029451868012\n"
     ]
    }
   ],
   "source": [
    "random_rfc_class_test_scores = random_rfc_class.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', random_rfc_class_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the accuracy of the models on the test dataset, it seems that the Multinomial Naive Bayes had the best predictive performance with a 0.776 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this assignment was to optimize the basic text classification pipeline of the 20newsgroup dataset. Thus, i started first by testing 12 different preprocessing combinations in attempt to find the best preprocessing technique in addition to determining the preprocessing step which has the highest impact on the models' performance. The two main libraries that were used are Spacy and NLTK. Moreover, i also compared the lemmatization and the stemming techniques to see which one had a better effect on the models's accuracy. It's important to note that while testing different preprocessing combinations, TF-IDF was used as the default feature extraction techniques for all of the combinations. Besides, the preprocessing techniques were tested on six different classification algorithms (Naive Bayes, Logistic Regression, Support Vector Classifier, Stochastic Gradient Descent, Random Foreset, and a Voting Classifier which included all the previous algorithms).\n",
    "\n",
    "Next after finding the best sequence of preprocessing techniques, i resplit the data into three different sets; training, validation, and testing. I did so because i found it more convenient to use a new notebook for the feature extraction task. I tested six different feature extraction techniques which are the following: \n",
    "\n",
    "               1- Bag of Words (Unigrams)\n",
    "               2- Bag of Words (Unigrams and Bigrams)\n",
    "               3- TF-IDF (Unigrams)\n",
    "               4- TF-IDF (Unigrams and Bigrams)\n",
    "               5- Bag of Words and TF-IDF (Unigrams and Bigrams)\n",
    "               6- Word2Vec and Glove (Unigrams and Bigrams)\n",
    "\n",
    "As before, those feature extraction techniques were tested on all six predefined models.\n",
    " \n",
    "After selecting the best preprocessing and feature extraction techniques, i then moved into hyperparameter tuning. I tuned the hyperparameters of seven different models using a Randomized Grid Search. The seven models are:\n",
    "\n",
    "               1- Multinomial Naive Bayes Classifier\n",
    "               2- Complement Naive Bayes Classifier\n",
    "               3- Logistic Regression Classifier\n",
    "               4- Support Vector Machine Classifier \n",
    "               5- Stochastic Gradient Descent Classifier\n",
    "               6 -Random Forest Classifier\n",
    "               7- K-Nearest Neighbors Classifier\n",
    "\n",
    "After finding the best hyperparameters for each of the seven models, i then moved to the final testing stage. Each of the seven models (which are now stored in the Grid Search object after setting 'refit = True') were tested against the testing dataset, and finally, the best model was chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing stage, the following combination was found to exhibit the highest prediction accuracy: \n",
    "                \n",
    "               1- HTML Stripping\n",
    "               2- Contraction Expansion\n",
    "               3- Text Lower Case\n",
    "               4- Text Lemmatization\n",
    "               5- Stopword Removal\n",
    "               6- Accented Char Removal\n",
    "               7- Special Char Removal\n",
    "               \n",
    "As for the feature extraction stage, the TF-IDF with Unigrams and Bigrams seemed to have the highest effect on model performace as thus was picked.\n",
    "\n",
    "Next, for the hyperparameter tuning, Multinomial Naive Bayes performed best with an alpha = 0.01. The Complement Naive Bayes performed best with an alpha = 0.01 also. As for the Logistic Regression, the model performed best with an l2 penalty, C = 100, and solver = liblinear. SVM did best with a kernel = linear and C = 10. SGD had the highest accuracy with a penalty = l2 and alpha = 0.0001. As for Random Forest, it did best with n-estimators = 200 and criterion = gini. Lastly, the best parameters for the KNN were n-neighbors = 200 and metric = euclidean.\n",
    "\n",
    "Finally, after testing the models against the testing dataset, it was found the Multinomial Naive Bayes performed best achieving an accuracy of 0.776. \n",
    "\n",
    "Note: Due to the high computational cost, i wasnt able to test all the hyperparameters values, and thus was obliged to run the Grid Search only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Redo the hyperparameter stage by including more parameters and a bigger range of values\n",
    "\n",
    "2- The data needs more preprocessing as the corpus is still a bit messy and the words arent completely tockenized\n",
    "\n",
    "3- More models should be tested, especially Neural Networks and RNN which are proved to perform very well on such type of problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I, Samer Haidar, affirm that I completed this assignment on my own without receiving or giving any help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
